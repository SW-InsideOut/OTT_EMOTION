"""
import cv2
import numpy as np
import tensorflow as tf
import time

# ğŸ¯ Haar Cascade ë¡œë“œ (ì •ë©´ ì–¼êµ´ ê²€ì¶œ)
face_cascade = cv2.CascadeClassifier("../haarcascade_frontalface_default.xml")

# ğŸ¯ ê°ì • ë¶„ì„ ëª¨ë¸ ë¡œë“œ
MODEL_PATH = "../models/best_model_local8.h5"  # ì‹¤ì œ ëª¨ë¸ íŒŒì¼ ê²½ë¡œë¥¼ ì§€ì •í•˜ì„¸ìš”
model = tf.keras.models.load_model(MODEL_PATH)

# ê°ì • í´ë˜ìŠ¤ ë¼ë²¨ (ìˆœì„œê°€ ì¤‘ìš”í•¨!)
class_labels = ['angry', 'happy', 'neutral', 'sad', 'surprize']

# ğŸ¥ ì›¹ìº  ì—´ê¸° (0ë²ˆ: ê¸°ë³¸ ì¹´ë©”ë¼)
cap = cv2.VideoCapture(0)

# ğŸ¯ ìƒíƒœ ê´€ë¦¬ ë³€ìˆ˜
current_emotion = "neutral"  # í˜„ì¬ í‘œì‹œí•  ê°ì •
previous_emotion = "neutral"  # ì§ì „ ê°ì •
emotion_change_time = time.time()  # ê°ì • ë°”ë€ ì‹œê°„ ê¸°ë¡

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=6, minSize=(60, 60))

    for (x, y, w, h) in faces:
        face = gray[y:y+h, x:x+w]
        face_resized = cv2.resize(face, (48, 48)) / 255.0
        face_reshaped = np.expand_dims(face_resized, axis=(0, -1))

        predictions = model.predict(face_reshaped, verbose=0)
        predicted_class = np.argmax(predictions)
        new_emotion = class_labels[predicted_class]

        now = time.time()

        # âœ… neutral â¡ï¸ ë‹¤ë¥¸ ê°ì •: ë°”ë¡œ ë°˜ì˜
        if previous_emotion == "neutral" and new_emotion != "neutral":
            current_emotion = new_emotion
            emotion_change_time = now
            previous_emotion = new_emotion  # ğŸ‘‰ ê°ì • ì „í™˜ ì‹œì—ë§Œ ì—…ë°ì´íŠ¸
            print(f"ğŸ”„ Neutral â¡ï¸ {new_emotion}")

        # âœ… ë‹¤ë¥¸ ê°ì • â¡ï¸ ë‹¤ë¥¸ ê°ì •: 3ì´ˆ ìœ ì§€
        elif previous_emotion != "neutral" and new_emotion != previous_emotion:
            if now - emotion_change_time > 3:
                current_emotion = new_emotion
                emotion_change_time = now
                previous_emotion = new_emotion  # ğŸ‘‰ ê°ì • ì „í™˜ ì‹œì—ë§Œ ì—…ë°ì´íŠ¸
                print(f"â±ï¸ ê°ì • ìœ ì§€ í›„ ë³€ê²½: {previous_emotion} â¡ï¸ {new_emotion}")

        # âœ… ê°ì •ì´ ìœ ì§€ë˜ê±°ë‚˜ neutral ê°ì •ì¼ ë•Œ: ì—…ë°ì´íŠ¸ (previous_emotion ìœ ì§€!)
        elif new_emotion == previous_emotion or new_emotion == "neutral":
            current_emotion = new_emotion
            emotion_change_time = now
            # âŒ previous_emotion ì€ ì—¬ê¸°ì„œ ì—…ë°ì´íŠ¸í•˜ì§€ ì•ŠìŒ

        # ğŸ”¹ ì–¼êµ´ ë° ê°ì • ë¼ë²¨ í‘œì‹œ
        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)
        cv2.putText(frame, current_emotion, (x, y-10),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)

    # í™”ë©´ ì¶œë ¥
    cv2.imshow("Real-time Emotion Detection", frame)

    if cv2.waitKey(1) & 0xFF == 27:  # ESC
        break

cap.release()
cv2.destroyAllWindows()
"""
import cv2
import numpy as np
import tensorflow as tf
import time

# ğŸ¯ Haar Cascade ë¡œë“œ (ì •ë©´ ì–¼êµ´ ê²€ì¶œ)
face_cascade = cv2.CascadeClassifier("../haarcascade_frontalface_default.xml")

# ğŸ¯ ê°ì • ë¶„ì„ ëª¨ë¸ ë¡œë“œ
MODEL_PATH = "../models/best_model_local8.h5"
model = tf.keras.models.load_model(MODEL_PATH)

# ê°ì • í´ë˜ìŠ¤ ë¼ë²¨
class_labels = ['angry', 'happy', 'neutral', 'sad', 'surprize']

# ğŸ¥ ì›¹ìº  ì—´ê¸°
cap = cv2.VideoCapture(0)

# ğŸ¯ ìƒíƒœ ê´€ë¦¬ ë³€ìˆ˜
current_emotion = "neutral"
previous_emotion = "neutral"
emotion_change_time = time.time()

while cap.isOpened():
    frame_start = time.time()  # ğŸ“¸ í”„ë ˆì„ ì‹œì‘ ì‹œê°„

    ret, frame = cap.read()
    if not ret:
        break

    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=6, minSize=(60, 60))

    for (x, y, w, h) in faces:
        face = gray[y:y+h, x:x+w]
        face_resized = cv2.resize(face, (48, 48)) / 255.0
        face_reshaped = np.expand_dims(face_resized, axis=(0, -1))

        # â±ï¸ ê°ì • ì˜ˆì¸¡ ì‹œê°„ ì¸¡ì •
        pred_start = time.time()
        predictions = model.predict(face_reshaped, verbose=0)
        pred_end = time.time()
        print(f"â±ï¸ ê°ì • ë¶„ë¥˜ ì†Œìš” ì‹œê°„: {pred_end - pred_start:.4f}ì´ˆ")

        predicted_class = np.argmax(predictions)
        new_emotion = class_labels[predicted_class]

        now = time.time()

        # ğŸ”„ ê°ì • ìƒíƒœ ì „í™˜ ë¡œì§
        if previous_emotion == "neutral" and new_emotion != "neutral":
            current_emotion = new_emotion
            emotion_change_time = now
            previous_emotion = new_emotion
            print(f"ğŸ”„ Neutral â¡ï¸ {new_emotion}")

        elif previous_emotion != "neutral" and new_emotion != previous_emotion:
            if now - emotion_change_time > 3:
                current_emotion = new_emotion
                emotion_change_time = now
                previous_emotion = new_emotion
                print(f"â±ï¸ ê°ì • ìœ ì§€ í›„ ë³€ê²½: {previous_emotion} â¡ï¸ {new_emotion}")

        elif new_emotion == previous_emotion or new_emotion == "neutral":
            current_emotion = new_emotion
            emotion_change_time = now

        # ì–¼êµ´ ë° ê°ì • ë¼ë²¨ í‘œì‹œ
        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)
        cv2.putText(frame, current_emotion, (x, y-10),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)

    # í™”ë©´ ì¶œë ¥
    cv2.imshow("Real-time Emotion Detection", frame)

    # ğŸ“¸ FPS ì¸¡ì •
    frame_end = time.time()
    fps = 1.0 / (frame_end - frame_start)
    print(f"ğŸ“¸ í”„ë ˆì„ ì²˜ë¦¬ ì†ë„: {fps:.2f} FPS")

    if cv2.waitKey(1) & 0xFF == 27:  # ESC í‚¤ ì¢…ë£Œ
        break

cap.release()
cv2.destroyAllWindows()
